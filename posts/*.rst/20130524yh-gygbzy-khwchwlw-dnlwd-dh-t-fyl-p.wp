<html><body><p>دیروز، کسی از گوگل پلاس، که یادم نیست کی بود، <a href="/paper-tools/trakt-nevisi">این لینک رو از ستاد سلام</a> شیر کرده بود. درمورد چیزایی مثل حقوق زنان هست. می‌خواستم تمام فایل‌های پی‌دی‌اف رو دانلود بکنم، ولی در شأن یه گیک نیست که تک به تک بشینه فایل‌ها رو دانلود بکنه ;) پس یکی دو خط کد می‌زنیم.<br>
اول، باید یه فایل اچ‌تی‌ام‌ال از اون صفحه ستاد سلام بگیرم:</p>

<p dir="ltr">wget "<a href="/paper-tools/trakt-nevisi">http://www.setadsalam.net/paper-tools/trakt-nevisi</a>" -O trakt_nevisi.html</p>

<p>حالا، باید لینک‌ها رو از فایل جدا کنیم و بذاریم تو یه فایل دیگه:</p>

<p dir="ltr">cat trakt_nevisi.html | grep -Po 'href="(.*?)\.pdf"' | cut -d\" -f2 | uniq | sed "s|^|<a href="http://www.setadsalam.net">http://www.setadsalam.net</a>|g" | tee URLs</p>

<p>خلاصه این کد اینه که اول با grep و cut آدرس‌ها رو از فایل جدا می‌کنیم، بعد چون هر آدرس یه بار هم تکرار شده، تکراری‌ها رو حذف می‌کنیم و بعد، به خاطر این که اولشون اسلش (/) هست و نه آدرس سایت، آدرس ستاد سلام رو هم به اولش اضافه می‌کنیم. بعدش هم با tee آدرس‌ها رو تو فایل URLs میذاریم.<br>

خوب، حالا فقط دانلودشون مونده:</p>

<p dir="ltr">wget -i URLs</p>

<p>با آپشن ‎-i به wget میفهمونیم که باید آدرس‌ها رو از فایل URLs بگیره.<br>

حالا برای اطمینان، تعداد فایل‌ها رو بررسی می‌کنیم:</p>

<p dir="ltr">‎ $ ls *.pdf | wc -l<br>

‎10</p>

<p>خوب، ده تا کلیک صرفه‌جویی شد ;)</p>

<p>پی‌نوشت: این پست و <a href="/2013/05/23/%D8%AF%D9%88%D8%B1%D9%87-%D8%AC%D8%AF%DB%8C%D8%AF-%D8%AF%D8%B1%D8%B3%D9%86%D8%A7%D9%85%D9%87-%D8%A8%D8%B1%D8%A7%DB%8C-%D8%AE%D9%88%D8%A7%D9%86%D9%86%D8%AF%D9%87%E2%80%8C%D9%87%D8%A7-%D9%88-%D9%BE/">پست قبلی</a> رو با ایمیل به وردپرس فرستادم. واسه همین تا وقتی که بتونم سد فیلترینگ رو رد بکنم، این دو مطلب و شاید مطالب بعدی رو به دسته خاصی نمی‌تونم اضافه بکنم.</p>

</body></html>
